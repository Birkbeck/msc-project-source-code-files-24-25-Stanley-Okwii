{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c897fca",
   "metadata": {},
   "source": [
    "# Model training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e8ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abae82b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using available GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 300\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "EPOCHS = 5\n",
    "INPUT_SHAPE=(224, 224, 3)\n",
    "\n",
    "tf.random.set_seed(5)\n",
    "dataset_dir = \"../datasets\"\n",
    "\n",
    "# Change dataset_dir when run in google colab \n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    dataset_dir = \"/content/drive/Othercomputers/Big Mac/datasets\"\n",
    "    BATCH_SIZE = 430\n",
    "\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Using available GPUs: \", physical_gpus)\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73b5987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image count: 128116\n",
      "Validation image count: 25000\n",
      "Test image count: 25000\n"
     ]
    }
   ],
   "source": [
    "# Load ImageNet2012 dataset\n",
    "def prepare_input_data(input):\n",
    "    image = tf.cast(input['image'], tf.float32)\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = preprocess_input(image)\n",
    "    label = input['label']\n",
    "    return image, label\n",
    "\n",
    "def make_dataset(ds):\n",
    "    return (\n",
    "        ds.map(prepare_input_data, num_parallel_calls=AUTOTUNE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "(train, validation, test), info = tfds.load(\n",
    "    'imagenet2012_subset/10pct',\n",
    "    split=['train', 'validation[:50%]', 'validation[50%:]'],\n",
    "    shuffle_files=False,\n",
    "    with_info=True,\n",
    "    data_dir=dataset_dir\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "class_names = info.features['label'].names\n",
    "\n",
    "print(f\"Train image count: {info.splits['train'].num_examples}\")\n",
    "print(f\"Validation image count: {info.splits['validation[:50%]'].num_examples}\")\n",
    "print(f\"Test image count: {info.splits['validation[50%:]'].num_examples}\")\n",
    "\n",
    "train_dataset = make_dataset(train)\n",
    "validation_dataset = make_dataset(validation)\n",
    "test_dataset = make_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a457f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 140 TFrecord train files\n",
      "Loaded 59 TFrecord test files\n"
     ]
    }
   ],
   "source": [
    "# Load adversarial datasets\n",
    "\n",
    "def _parse_image(input):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(input, feature_description)\n",
    "    image_f16 = tf.io.parse_tensor(parsed_features['image'], out_type=tf.float16)\n",
    "    label = parsed_features['label']\n",
    "    image_f32 = tf.cast(image_f16, tf.float32)\n",
    "    image_f32.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
    "    return image_f32, label\n",
    "\n",
    "def create_tf_dataset(file_paths):\n",
    "    raw_dataset = tf.data.TFRecordDataset(file_paths, compression_type='GZIP')\n",
    "    tf_dataset = raw_dataset.map(_parse_image).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return tf_dataset\n",
    "\n",
    "# Get all adversarial datasets for train, and testing\n",
    "test_file_paths = glob.glob(f'{dataset_dir}/adversaries/imagenet2012_subset/test-*.tfrec')\n",
    "train_file_paths = glob.glob(f'{dataset_dir}/adversaries/imagenet2012_subset/train-*.tfrec')\n",
    "\n",
    "print(f\"Loaded {len(train_file_paths)} TFrecord train files\")\n",
    "print(f\"Loaded {len(test_file_paths)} TFrecord test files\")\n",
    "\n",
    "# Create a TFRecordDataset\n",
    "adv_test_dataset = create_tf_dataset(test_file_paths)\n",
    "adv_train_dataset = create_tf_dataset(train_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7714d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded robust resnet mode\n"
     ]
    }
   ],
   "source": [
    "# Load robust model from saved file\n",
    "print(\"loaded robust model\")\n",
    "robust_model = tf.keras.models.load_model(\"robust_resnet50.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e7e718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating training accuracy...\n",
      "\n",
      "\u001b[1m161/428\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4:51\u001b[0m 1s/step - accuracy: 9.8640e-04 - loss: 1.4088 - top_5_accuracy: 0.0090"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Collection of metrics\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating training accuracy...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train_metrics = \u001b[43mrobust_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating adversarial accuracy...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m adv_metrics = robust_model.evaluate(adv_test_dataset, batch_size=BATCH_SIZE, verbose=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:489\u001b[39m, in \u001b[36mTensorFlowTrainer.evaluate\u001b[39m\u001b[34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    488\u001b[39m     callbacks.on_test_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m     callbacks.on_test_batch_end(end_step, logs)\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_evaluating:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1498\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1508\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1509\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1510\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1514\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1515\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Collection of metrics\n",
    "\n",
    "print(\"Evaluating training accuracy...\\n\")\n",
    "train_metrics = robust_model.evaluate(train_dataset, batch_size=BATCH_SIZE ,verbose=1)\n",
    "\n",
    "print(\"Evaluating adversarial accuracy...\\n\")\n",
    "adv_metrics = robust_model.evaluate(adv_test_dataset, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "print(\"Evaluating standard accuracy...\\n\")\n",
    "test_metrics = robust_model.evaluate(test_dataset,batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa978ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze metrics\n",
    "\n",
    "metric_names = ['loss', 'accuracy', 'top_5_accuracy']\n",
    "\n",
    "# Unpack metrics directly from the evaluation results\n",
    "train_loss, train_acc, train_top5 = train_metrics\n",
    "test_loss, test_acc, test_top5 = test_metrics\n",
    "adv_loss, adv_acc, adv_top5 = adv_metrics\n",
    "\n",
    "# --- METRIC CALCULATIONS ---\n",
    "# Generalization Gap: Difference between training and test accuracy.\n",
    "generalization_gap = train_acc - test_acc\n",
    "\n",
    "# Robustness/Transferability Gap: Difference between accuracy on clean and adversarial data.\n",
    "robustness_gap = test_acc - adv_acc\n",
    "\n",
    "print(\"# Standard Accuracy\")\n",
    "print(f\"Top 1 Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Top 5 Accuracy: {test_top5*100:.2f}%\")\n",
    "print(f\"Loss: {test_loss*100:.2f}%\\n\")\n",
    "\n",
    "print(\"# Generalization\")\n",
    "print(f\"Training Accuracy: {train_acc*100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Generalization Gap: {generalization_gap*100:.2f}%\\n\")\n",
    "\n",
    "print(\"# Robustness & Transferability\")\n",
    "print(f\"Standard Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Adversarial Accuracy: {adv_acc*100:.2f}%\")\n",
    "print(f\"Robustness (Transferability) Gap: {robustness_gap*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c434df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-talk')\n",
    "\n",
    "# Grouped data\n",
    "labels = ['Training', 'Standard Test', 'Adversarial Test']\n",
    "top1_accuracies = [train_acc, test_acc, adv_acc]\n",
    "top5_accuracies = [train_top5, test_top5, adv_top5]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "# Create Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot bars for Top-1 and Top-5 accuracy\n",
    "rects1 = ax.bar(x - width/2, top1_accuracies, width, label='Top-1 Accuracy', color='#4c72b0', zorder=2)\n",
    "rects2 = ax.bar(x + width/2, top5_accuracies, width, label='Top-5 Accuracy', color='#55a868', zorder=2)\n",
    "\n",
    "# A helper function to attach a text label above each bar\n",
    "def add_labels(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.1%}', # Format as percentage with 1 decimal place\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "add_labels(rects1)\n",
    "add_labels(rects2)\n",
    "\n",
    "# Set a clear title\n",
    "ax.set_title('Top-1 vs. Top-5 Accuracy across datasets', fontsize=16, pad=30)\n",
    "\n",
    "# Set y-axis label and limits\n",
    "ax.set_ylabel('Accuracy', fontsize=12, labelpad=15)\n",
    "ax.set_ylim(0, 1.19)\n",
    "\n",
    "# Format the y-axis to show percentages directly\n",
    "ax.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0))\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, fontsize=12)\n",
    "\n",
    "# Add a light horizontal grid for easier value reading\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add a legend to differentiate the bars\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Adjust layout to prevent labels from being cut off\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57329bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def get_imagenet_readable_labels():\n",
    "    \"\"\"\n",
    "    Fetches the 1000 ImageNet class labels.\n",
    "    Returns a list of class names or None if fetching fails.\n",
    "    \"\"\"\n",
    "    url = \"https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # The JSON is a dictionary like: {\"0\": [\"n01440764\", \"tench\"], ...}\n",
    "        class_data = response.json()\n",
    "        \n",
    "        class_names = [class_data[str(i)][1] for i in range(len(class_data))]\n",
    "\n",
    "        print(\"Labels fetched...\")\n",
    "        return class_names\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching labels: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Fetch the class labels once\n",
    "class_names = get_imagenet_readable_labels()\n",
    "\n",
    "\n",
    "def plot_misclassified_cm(model, dataset, class_names, k=10, use_class_names=None):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix for the top-k most misclassified classes.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to evaluate.\n",
    "        dataset: The dataset to evaluate on.\n",
    "        class_names: A list of all possible class names.\n",
    "        k (int): The number of top classes to display.\n",
    "        use_class_names (list, optional): If provided, the plot will use this\n",
    "            exact list of class names instead of calculating the top-k from the\n",
    "            current dataset. This is useful for comparing performance on the\n",
    "            same subset of classes across different datasets (e.g., standard vs. adversarial).\n",
    "\n",
    "    Returns:\n",
    "        list: The list of the top-k misclassified class names found in the dataset.\n",
    "              Returns None if use_class_names was provided.\n",
    "    \"\"\"\n",
    "    if not class_names:\n",
    "        print(\"Cannot generate plot without class names.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nComputing model predictions...\")\n",
    "    y_true = np.concatenate([y.numpy() for _, y in dataset], axis=0)\n",
    "    y_pred_proba = model.predict(dataset, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "    # --- Calculate the full confusion matrix ---\n",
    "    cm = confusion_matrix(y_true, y_pred_classes, labels=np.arange(len(class_names)))\n",
    "\n",
    "    print(\"\\n Overall Performance Metrics\")\n",
    "    report = classification_report(y_true, y_pred_classes, output_dict=True, zero_division=0)\n",
    "    macro_avg = report['macro avg']\n",
    "    print(f\"Macro Precision: {macro_avg['precision']:.4f}\")\n",
    "    print(f\"Macro Recall:    {macro_avg['recall']:.4f}\")\n",
    "    print(f\"Macro F1-Score:  {macro_avg['f1-score']:.4f}\")\n",
    "\n",
    "    # --- Determine which classes to plot ---\n",
    "    if use_class_names is None:\n",
    "        # This is the \"discovery\" mode: find the top-k misclassified classes\n",
    "        print(f\"Identifying the top {k} most misclassified classes...\")\n",
    "        misclassification_counts = cm.sum(axis=1) - np.diag(cm)\n",
    "        top_k_indices = np.argsort(misclassification_counts)[-k:][::-1]\n",
    "        \n",
    "        # Get the names for the identified classes\n",
    "        plot_class_names = [class_names[i] for i in top_k_indices]\n",
    "        \n",
    "        color_map = 'Greens'\n",
    "        heatmap_title = 'Standard'\n",
    "\n",
    "        for i in top_k_indices:\n",
    "            total_samples = cm.sum(axis=1)[i]\n",
    "            accuracy = (np.diag(cm)[i] / total_samples) if total_samples > 0 else 0\n",
    "            print(f\"Class: {class_names[i]:<20s} | Misclassified: {misclassification_counts[i]}/{total_samples:<4} | Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "        return_value = plot_class_names # We will return the names we found\n",
    "    else:\n",
    "        # This is the \"comparison\" mode: use the provided class names\n",
    "        print(f\"Analyzing performance on a pre-defined set of {len(use_class_names)} classes...\")\n",
    "        plot_class_names = use_class_names\n",
    "        \n",
    "        # Find the indices corresponding to the provided class names\n",
    "        class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "        top_k_indices = [class_to_idx[name] for name in plot_class_names if name in class_to_idx]\n",
    "        \n",
    "        color_map = 'Reds'\n",
    "        heatmap_title = 'Adversarial'\n",
    "\n",
    "        misclassification_counts = cm.sum(axis=1) - np.diag(cm)\n",
    "        for i in top_k_indices:\n",
    "            total_samples = cm.sum(axis=1)[i]\n",
    "            accuracy = (np.diag(cm)[i] / total_samples) if total_samples > 0 else 0\n",
    "            print(f\"Class: {class_names[i]:<20s} | Misclassified: {misclassification_counts[i]}/{total_samples:<4} | Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "        return_value = None # We don't need to return anything in this mode\n",
    "\n",
    "\n",
    "    # Create the reduced confusion matrix for plotting\n",
    "    cm_top_k = cm[np.ix_(top_k_indices, top_k_indices)]\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        cm_top_k, annot=True, fmt='d', cmap=color_map,\n",
    "        xticklabels=plot_class_names, yticklabels=plot_class_names\n",
    "    )\n",
    "    plt.title(f'{heatmap_title} Confusion Matrix: Top {k} Misclassified Classes', fontsize=16, pad=20)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "\n",
    "print(\"Running confusion matrix for standard test\")\n",
    "# First call: Discover the top 10 misclassified classes on the standard test set\n",
    "# and store the returned list of names in a variable.\n",
    "top_10_misclassified_classes = plot_misclassified_cm(base_model, test_dataset, class_names, k=10)\n",
    "\n",
    "print(\"Running confusion matrix for adversarial test\")\n",
    "# Second call: Pass the list from the first run to the `use_class_names` parameter.\n",
    "if top_10_misclassified_classes:\n",
    "    plot_misclassified_cm(base_model, adv_test_dataset, class_names, k=10, use_class_names=top_10_misclassified_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
