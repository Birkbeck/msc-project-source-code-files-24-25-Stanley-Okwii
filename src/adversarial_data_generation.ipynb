{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dab9466",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "While from imagenet might need little processing. To retrain Resnet-50 with adversarial inputs, they need to be generated. Training dataset has about 1M images, preprocessing, generates adversarial input for both training and testing datasets.\n",
    "Corresponding adversarial tensorflow records are created for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d4428a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using GPU with 55GB of memory\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "# BATCH_SIZE = 5000\n",
    "BATCH_SIZE = 200\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "EPOCHS = 5\n",
    "tf.random.set_seed(5)\n",
    "\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", physical_gpus)\n",
    "\n",
    "try:\n",
    "    tf.keras.mixed_precision.set_global_policy('float32')\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        physical_gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=56320)]  # Limit RAM to 55GB to avoid starving PC\n",
    "    )\n",
    "    print(\"Using GPU with 55GB of memory\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4040305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image count: 9469\n",
      "Test image count: 3925\n"
     ]
    }
   ],
   "source": [
    "# Load ImageNet data\n",
    "\n",
    "def prepare_input_data(input):\n",
    "    image = tf.cast(input['image'], tf.float32) # ResNet-50 used this\n",
    "    image = tf.image.resize(input['image'], (IMG_SIZE, IMG_SIZE))\n",
    "    image = preprocess_input(image)\n",
    "    label = input['label']\n",
    "    return image, label\n",
    "\n",
    "# Big dataset for real work\n",
    "# dataset, info = tfds.load(\n",
    "#     'imagenet2012',\n",
    "#     shuffle_files=False,\n",
    "#     with_info=True,\n",
    "#     data_dir='../datasets'\n",
    "# )\n",
    "\n",
    "# Smaller dataset for testing\n",
    "dataset, info = tfds.load(\n",
    "    'imagenette',\n",
    "    shuffle_files=False,\n",
    "    with_info=True,\n",
    "    data_dir='../datasets'\n",
    ")\n",
    "\n",
    "# Dataset stats\n",
    "print(f'Train image count: {info.splits['train'].num_examples}')\n",
    "print(f'Test image count: {info.splits['validation'].num_examples}')\n",
    "\n",
    "# Preprocess data\n",
    "train_dataset = dataset['train'].map(prepare_input_data, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "test_dataset = dataset['validation'].map(prepare_input_data, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dbe3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet50 model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# base_model = ResNet50(\n",
    "#     include_top=True,\n",
    "#     weights='imagenet',\n",
    "#     input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "#     pooling=None,\n",
    "#     classes=1000,\n",
    "#     classifier_activation='softmax'\n",
    "# )\n",
    "base_model = ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,  # Don't load pretrained weights since we're changing the output\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling=None,\n",
    "    classes=10,  # Imagenette has 10 classes\n",
    "    classifier_activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a0c8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_processed_dataset('../datasets/adversaries/test_dataset.tfrecord', adversarial_test_dataset)\n",
    "# %reset_selective -f adversarial_test_dataset test_dataset\n",
    "# import gc;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28a298a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # get value from EagerTensor\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _create_adversary_with_pgd(model, images, labels, eps, eps_iter, nb_iter):\n",
    "    \"\"\"\n",
    "    This generates adversarial images by iteratively applying a small\n",
    "    perturbation in the direction of the gradient of the loss, and then\n",
    "    projecting the result back into the epsilon-ball of the original image.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The model to attack.\n",
    "        images (tf.Tensor): The original, clean input images.\n",
    "        labels (tf.Tensor): The true labels for the images.\n",
    "        eps (float): The maximum perturbation (L-infinity norm).\n",
    "        eps_iter (float): The step size for each attack iteration.\n",
    "        nb_iter (int): The number of PGD iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: The generated adversarial images.\n",
    "    \"\"\"\n",
    "    x_adv = tf.identity(images)\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    for _ in range(nb_iter):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            prediction = model(x_adv, training=False)\n",
    "            loss = loss_object(labels, prediction)\n",
    "\n",
    "        gradients = tape.gradient(loss, x_adv)\n",
    "        signed_grad = tf.sign(gradients)\n",
    "        x_adv = x_adv + eps_iter * signed_grad\n",
    "        perturbation = tf.clip_by_value(x_adv - images, -eps, eps)\n",
    "        x_adv = images + perturbation\n",
    "\n",
    "    return x_adv\n",
    "\n",
    "def generate_adversarial_dataset(filename,dataset, model, eps, pgd_steps, pgd_step_size):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples and saves them to a TFRecord file\n",
    "    by serializing the raw float32 tensors.\n",
    "    \"\"\"\n",
    "    options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "    num = 0\n",
    "    with tf.io.TFRecordWriter(filename, options=options) as writer:\n",
    "        for i, (images, labels) in enumerate(dataset):\n",
    "            print(f\"Batch {i+1}\")\n",
    "            # Generate the adversarial images (these are already preprocessed)\n",
    "            adv_images = _create_adversary_with_pgd(\n",
    "                model=model,\n",
    "                images=images,\n",
    "                labels=labels,\n",
    "                eps=eps,\n",
    "                eps_iter=pgd_step_size,\n",
    "                nb_iter=pgd_steps\n",
    "            )\n",
    "\n",
    "            # Iterate through the batch to save each image/label pair\n",
    "            for i in range(len(adv_images)):\n",
    "                image_tensor = adv_images[i]\n",
    "                label = labels[i]\n",
    "\n",
    "                # 1. Cast the tensor to float16 to halve its size\n",
    "                image_tensor_f16 = tf.cast(image_tensor, tf.float16)\n",
    "\n",
    "                # 2. Serialize the smaller tensor\n",
    "                image_bytes = tf.io.serialize_tensor(image_tensor_f16)\n",
    "                # 2. Create the feature and write to the TFRecord file\n",
    "                feature = {\n",
    "                    'image': _bytes_feature(image_bytes), # Save the raw serialized tensor\n",
    "                    'label': _int64_feature(label.numpy())\n",
    "                }\n",
    "                num += 1\n",
    "                serialized_example = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "                writer.write(serialized_example)\n",
    "\n",
    "    print(f\"Processed and saved: {num} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e703c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create adversarial dataset\n",
    "EPSILON = 0.03\n",
    "PGD_STEPS = 2\n",
    "PGD_STEP_SIZE = 0.007\n",
    "adversarial_test_file = '../datasets/adversaries/small_test_dataset.tfrec'\n",
    "# generate_adversarial_dataset(\n",
    "#     filename=adversarial_test_file,\n",
    "#     dataset=test_dataset,\n",
    "#     model=base_model,\n",
    "#     eps=EPSILON,\n",
    "#     pgd_steps=PGD_STEPS,\n",
    "#     pgd_step_size=PGD_STEP_SIZE\n",
    "# )\n",
    "adversarial_train_file = '../datasets/adversaries/small_train_dataset.tfrec'\n",
    "# generate_adversarial_dataset(\n",
    "#     filename=adversarial_train_file,\n",
    "#     dataset=train_dataset,\n",
    "#     model=base_model,\n",
    "#     eps=EPSILON,\n",
    "#     pgd_steps=PGD_STEPS,\n",
    "#     pgd_step_size=PGD_STEP_SIZE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8c1b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data from file\n",
    "def _parse_function(proto):\n",
    "    \"\"\"\n",
    "    Parses a single example proto by deserializing the float16 tensor\n",
    "    and casting it back to float32.\n",
    "    \"\"\"\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
    "\n",
    "    # 1. Deserialize the byte string back into a float16 tensor\n",
    "    image_f16 = tf.io.parse_tensor(parsed_features['image'], out_type=tf.float16)\n",
    "    label = parsed_features['label']\n",
    "\n",
    "    # 2. Cast the image back to float32 for the model\n",
    "    image_f32 = tf.cast(image_f16, tf.float32)\n",
    "\n",
    "    # 3. Set the shape on the final float32 tensor\n",
    "    image_f32.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
    "\n",
    "    return image_f32, label\n",
    "\n",
    "# Load the TFRecord file back into a dataset\n",
    "loaded_test_dataset = tf.data.TFRecordDataset(adversarial_test_file, compression_type='GZIP')\n",
    "\n",
    "# Map the parsing function across the dataset\n",
    "parsed_test_dataset = loaded_test_dataset.map(_parse_function).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ee463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# First, update the model compilation to include more metrics\n",
    "base_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        # tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy'),\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='sparse_categorical_accuracy'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        # tf.keras.metrics.Precision(average='macro', name='precision'),\n",
    "        # tf.keras.metrics.Recall(average='macro', name='recall')\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Training the model on clean data...\\n\")\n",
    "history = base_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=5,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db083dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on all datasets\n",
    "print(\"Computing baseline metrics...\\n\")\n",
    "\n",
    "train_metrics = base_model.evaluate(train_dataset, verbose=1)\n",
    "test_metrics = base_model.evaluate(test_dataset, verbose=1)\n",
    "noisy_metrics = base_model.evaluate(parsed_test_dataset, verbose=1)\n",
    "\n",
    "# Extract metrics\n",
    "# train_loss, train_acc, train_top5, train_auc, train_prec, train_rec = train_metrics\n",
    "# test_loss, test_acc, test_top5, test_auc, test_prec, test_rec = test_metrics\n",
    "# noisy_loss, noisy_acc, noisy_top5, noisy_auc, noisy_prec, noisy_rec = noisy_metrics\n",
    "\n",
    "try:\n",
    "    # Extract metrics with error handling\n",
    "    metric_names = ['loss', 'accuracy', 'top_5_accuracy', 'sparse_categorical_accuracy', 'auc', 'precision', 'recall']\n",
    "    metrics_dict = {\n",
    "        'train': dict(zip(metric_names, train_metrics)),\n",
    "        'test': dict(zip(metric_names, test_metrics)),\n",
    "        'noisy': dict(zip(metric_names, noisy_metrics))\n",
    "    }\n",
    "\n",
    "    # Unpack metrics safely\n",
    "    train_loss, train_acc = metrics_dict['train']['loss'], metrics_dict['train']['accuracy']\n",
    "    train_top5, train_auc = metrics_dict['train']['top_5_accuracy'], metrics_dict['train']['auc']\n",
    "    train_prec, train_rec = metrics_dict['train']['precision'], metrics_dict['train']['recall']\n",
    "\n",
    "    test_loss, test_acc = metrics_dict['test']['loss'], metrics_dict['test']['accuracy']\n",
    "    test_top5, test_auc = metrics_dict['test']['top_5_accuracy'], metrics_dict['test']['auc']\n",
    "    test_prec, test_rec = metrics_dict['test']['precision'], metrics_dict['test']['recall']\n",
    "\n",
    "    noisy_loss, noisy_acc = metrics_dict['noisy']['loss'], metrics_dict['noisy']['accuracy']\n",
    "    noisy_top5, noisy_auc = metrics_dict['noisy']['top_5_accuracy'], metrics_dict['noisy']['auc']\n",
    "    noisy_prec, noisy_rec = metrics_dict['noisy']['precision'], metrics_dict['noisy']['recall']\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting metrics: {e}\")\n",
    "    print(\"Available metrics:\", base_model.metrics_names)\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"       Model Performance Analysis\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"## Base Performance Metrics 📊\")\n",
    "print(\"**Evaluated on clean test dataset**\\n\")\n",
    "print(f\"* **Top-1 Accuracy**: `{test_acc*100:.2f}%`\")\n",
    "print(f\"* **Top-5 Accuracy**: `{test_top5*100:.2f}%`\")\n",
    "print(f\"* **AUC Score**: `{test_auc:.3f}`\")\n",
    "print(f\"* **Precision**: `{test_prec:.3f}`\")\n",
    "print(f\"* **Recall**: `{test_rec:.3f}`\")\n",
    "print(f\"* **Loss**: `{test_loss:.3f}`\")\n",
    "print(\"\\n\" + \"---\")\n",
    "\n",
    "print(\"\\n## Generalization Analysis 🧠\")\n",
    "print(\"**Comparing training vs test performance**\\n\")\n",
    "print(f\"* **Training Accuracy**: `{train_acc*100:.2f}%`\")\n",
    "print(f\"* **Test Accuracy**: `{test_acc*100:.2f}%`\")\n",
    "print(f\"* **Generalization Gap**: `{(train_acc-test_acc)*100:.2f}%`\")\n",
    "print(f\"* **Training Loss**: `{train_loss:.3f}`\")\n",
    "print(f\"* **Test Loss**: `{test_loss:.3f}`\")\n",
    "print(\"> *A smaller gap indicates better generalization*\")\n",
    "print(\"\\n\" + \"---\")\n",
    "\n",
    "print(\"\\n## Adversarial Robustness 🛡️\")\n",
    "print(\"**Evaluating model stability against adversarial inputs**\\n\")\n",
    "print(f\"* **Clean Data Accuracy**: `{test_acc*100:.2f}%`\")\n",
    "print(f\"* **Adversarial Data Accuracy**: `{noisy_acc*100:.2f}%`\")\n",
    "print(f\"* **Robustness Gap**: `{(test_acc-noisy_acc)*100:.2f}%`\")\n",
    "print(f\"* **Clean AUC**: `{test_auc:.3f}`\")\n",
    "print(f\"* **Adversarial AUC**: `{noisy_auc:.3f}`\")\n",
    "print(\"> *Smaller gaps between clean and adversarial metrics indicate better robustness*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779eed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_processed_dataset('../datasets/adversaries/train_dataset.tfrecord', adversarial_train_dataset)\n",
    "# %reset_selective -f adversarial_train_dataset train_dataset\n",
    "# import gc;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d863ee-653d-4908-a016-3b4fe06b5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adversarial training\n",
    "# Load the TFRecord file back into a dataset\n",
    "loaded_train_dataset = tf.data.TFRecordDataset(adversarial_train_file, compression_type='GZIP')\n",
    "\n",
    "# Map the parsing function across the dataset\n",
    "parsed_train_dataset = loaded_train_dataset.map(_parse_function).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "history = base_model.fit(\n",
    "    parsed_train_dataset,\n",
    "    epochs=5, # Use a suitable number of epochs for your task\n",
    "    validation_data=parsed_test_dataset,\n",
    "    verbose=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e221e0-59ce-4aea-a188-364b179eccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing comprehensive metrics...\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 1 but is rank 0 for '{{node in_top_k/InTopKV2}} = InTopKV2[T=DT_INT32](resnet50_1/predictions_1/Softmax, ArgMax_1, in_top_k/InTopKV2/k)' with input shapes: [?,1000], [], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Evaluate model on all datasets\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComputing comprehensive metrics...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m train_metrics = \u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m test_metrics = base_model.evaluate(test_dataset, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m     22\u001b[39m noisy_metrics = base_model.evaluate(parsed_test_dataset, verbose=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Birkbeck/Msc Project/msc-project-source-code-files-24-25-Stanley-Okwii/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/math.py:47\u001b[39m, in \u001b[36min_top_k\u001b[39m\u001b[34m(targets, predictions, k)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34min_top_k\u001b[39m(targets, predictions, k):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43min_top_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Shape must be rank 1 but is rank 0 for '{{node in_top_k/InTopKV2}} = InTopKV2[T=DT_INT32](resnet50_1/predictions_1/Softmax, ArgMax_1, in_top_k/InTopKV2/k)' with input shapes: [?,1000], [], []."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# First compile model with comprehensive metrics\n",
    "base_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate model on all datasets\n",
    "print(\"Computing comprehensive metrics...\\n\")\n",
    "\n",
    "train_metrics = base_model.evaluate(train_dataset, verbose=0)\n",
    "test_metrics = base_model.evaluate(test_dataset, verbose=0)\n",
    "noisy_metrics = base_model.evaluate(parsed_test_dataset, verbose=0)\n",
    "\n",
    "# Extract all metrics\n",
    "train_loss, train_acc, train_top5, train_auc, train_prec, train_rec = train_metrics\n",
    "test_loss, test_acc, test_top5, test_auc, test_prec, test_rec = test_metrics\n",
    "noisy_loss, noisy_acc, noisy_top5, noisy_auc, noisy_prec, noisy_rec = noisy_metrics\n",
    "\n",
    "# Create figure for multiple plots\n",
    "plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "metrics = ['Top-1 Acc', 'Top-5 Acc', 'AUC']\n",
    "clean_scores = [test_acc*100, test_top5*100, test_auc*100]\n",
    "noisy_scores = [noisy_acc*100, noisy_top5*100, noisy_auc*100]\n",
    "\n",
    "x = range(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], clean_scores, width, label='Clean Data', color='skyblue')\n",
    "plt.bar([i + width/2 for i in x], noisy_scores, width, label='Adversarial Data', color='lightcoral')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Performance Metrics Comparison')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "\n",
    "# 2. Precision-Recall Plot\n",
    "plt.subplot(2, 2, 2)\n",
    "metrics = ['Precision', 'Recall']\n",
    "clean_scores = [test_prec, test_rec]\n",
    "noisy_scores = [noisy_prec, noisy_rec]\n",
    "\n",
    "x = range(len(metrics))\n",
    "plt.bar([i - width/2 for i in x], clean_scores, width, label='Clean Data', color='skyblue')\n",
    "plt.bar([i + width/2 for i in x], noisy_scores, width, label='Adversarial Data', color='lightcoral')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision-Recall Comparison')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "\n",
    "# 3. Loss Comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(['Training', 'Testing', 'Adversarial'], \n",
    "        [train_loss, test_loss, noisy_loss],\n",
    "        color=['green', 'skyblue', 'lightcoral'])\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Comparison Across Datasets')\n",
    "\n",
    "# 4. Robustness Gap\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(['Generalization Gap', 'Robustness Gap'], \n",
    "        [(train_acc - test_acc)*100, (test_acc - noisy_acc)*100],\n",
    "        color=['skyblue', 'lightcoral'])\n",
    "plt.ylabel('Gap Percentage (%)')\n",
    "plt.title('Model Gaps Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed metrics report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"       Comprehensive Model Evaluation Results\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"## Base Performance Metrics 📊\")\n",
    "print(\"**Evaluated on clean test dataset**\\n\")\n",
    "print(f\"* **Top-1 Accuracy**: `{test_acc*100:.2f}%`\")\n",
    "print(f\"* **Top-5 Accuracy**: `{test_top5*100:.2f}%`\")\n",
    "print(f\"* **AUC Score**: `{test_auc:.3f}`\")\n",
    "print(f\"* **Precision**: `{test_prec:.3f}`\")\n",
    "print(f\"* **Recall**: `{test_rec:.3f}`\")\n",
    "print(f\"* **Loss**: `{test_loss:.3f}`\")\n",
    "print(\"\\n\" + \"---\")\n",
    "\n",
    "print(\"\\n## Generalization Analysis 🧠\")\n",
    "print(\"**Comparing training vs test performance**\\n\")\n",
    "print(f\"* **Training Accuracy**: `{train_acc*100:.2f}%`\")\n",
    "print(f\"* **Test Accuracy**: `{test_acc*100:.2f}%`\")\n",
    "print(f\"* **Generalization Gap**: `{(train_acc-test_acc)*100:.2f}%`\")\n",
    "print(f\"* **Training Loss**: `{train_loss:.3f}`\")\n",
    "print(f\"* **Test Loss**: `{test_loss:.3f}`\")\n",
    "print(\"> *A smaller gap indicates better generalization*\")\n",
    "print(\"\\n\" + \"---\")\n",
    "\n",
    "print(\"\\n## Adversarial Robustness 🛡️\")\n",
    "print(\"**Evaluating model stability against adversarial inputs**\\n\")\n",
    "print(f\"* **Clean Data Accuracy**: `{test_acc*100:.2f}%`\")\n",
    "print(f\"* **Adversarial Data Accuracy**: `{noisy_acc*100:.2f}%`\")\n",
    "print(f\"* **Robustness Gap**: `{(test_acc-noisy_acc)*100:.2f}%`\")\n",
    "print(f\"* **Clean AUC**: `{test_auc:.3f}`\")\n",
    "print(f\"* **Adversarial AUC**: `{noisy_auc:.3f}`\")\n",
    "print(\"> *Smaller gaps between clean and adversarial metrics indicate better robustness*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b6b44",
   "metadata": {},
   "source": [
    "# bbbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 0 = all logs, 1 = filter INFO, 2 = filter INFO & WARNING, 3 = filter INFO, WARNING & ERROR\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # use \"3\" to hide even ERROR logs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Silence TensorFlow's Python logger as well\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "# Silence absl logs that TF uses\n",
    "# import absl.logging\n",
    "# absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "# # If on TF 1.x APIs:\n",
    "# try:\n",
    "#     tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 200\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "EPOCHS = 5 # Changed to a lower number for demonstration if retraining is needed\n",
    "tf.random.set_seed(5)\n",
    "\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", physical_gpus)\n",
    "\n",
    "try:\n",
    "    tf.keras.mixed_precision.set_global_policy('float32') # Ensured float32 policy as per the original notebook\n",
    "    if physical_gpus: # Check if GPUs are available before setting virtual device\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            physical_gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=56320)]  # Limit RAM to 55GB to avoid starving PC\n",
    "        )\n",
    "        print(\"Using GPU with 55GB of memory\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Load ImageNet data\n",
    "def prepare_input_data(input):\n",
    "    image = tf.cast(input['image'], tf.float32)\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE)) # Corrected to use the casted image\n",
    "    image = preprocess_input(image)\n",
    "    label = input['label']\n",
    "    return image, label\n",
    "\n",
    "dataset, info = tfds.load(\n",
    "    'imagenette',\n",
    "    shuffle_files=False,\n",
    "    with_info=True,\n",
    "    data_dir='../datasets'\n",
    ")\n",
    "# dataset, info = tfds.load(\n",
    "#     'imagenet2012',\n",
    "#     shuffle_files=False,\n",
    "#     with_info=True,\n",
    "#     data_dir='../datasets'\n",
    "# )\n",
    "\n",
    "print(f'Train image count: {info.splits[\"train\"].num_examples}')\n",
    "print(f'Test image count: {info.splits[\"validation\"].num_examples}')\n",
    "\n",
    "train_dataset = dataset['train'].map(prepare_input_data, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "test_dataset = dataset['validation'].map(prepare_input_data, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# Load ResNet50 model\n",
    "base_model = ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling=None,\n",
    "    classes=10,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "\n",
    "# base_model = ResNet50(\n",
    "#     include_top=True,\n",
    "#     weights='imagenet',\n",
    "#     input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "#     pooling=None,\n",
    "#     classes=1000,\n",
    "#     classifier_activation='softmax'\n",
    "# )\n",
    "\n",
    "# Functions for adversarial data generation and loading\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _create_adversary_with_pgd(model, images, labels, eps, eps_iter, nb_iter):\n",
    "    x_adv = tf.identity(images)\n",
    "    # Use from_logits=False because classifier_activation='softmax' means model outputs probabilities\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    for _ in range(nb_iter):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            prediction = model(x_adv, training=False)\n",
    "            loss = loss_object(labels, prediction)\n",
    "\n",
    "        gradients = tape.gradient(loss, x_adv)\n",
    "        signed_grad = tf.sign(gradients)\n",
    "        x_adv = x_adv + eps_iter * signed_grad\n",
    "        perturbation = tf.clip_by_value(x_adv - images, -eps, eps)\n",
    "        x_adv = images + perturbation\n",
    "\n",
    "    return x_adv\n",
    "\n",
    "def generate_adversarial_dataset(filename, dataset, model, eps, pgd_steps, pgd_step_size):\n",
    "    options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "    num = 0\n",
    "    with tf.io.TFRecordWriter(filename, options=options) as writer:\n",
    "        for i, (images, labels) in enumerate(dataset):\n",
    "            print(f\"Batch {i+1}\")\n",
    "            adv_images = _create_adversary_with_pgd(\n",
    "                model=model,\n",
    "                images=images,\n",
    "                labels=labels,\n",
    "                eps=eps,\n",
    "                eps_iter=pgd_step_size,\n",
    "                nb_iter=pgd_steps\n",
    "            )\n",
    "\n",
    "            for i in range(len(adv_images)):\n",
    "                image_tensor = adv_images[i]\n",
    "                label = labels[i]\n",
    "                image_tensor_f16 = tf.cast(image_tensor, tf.float16)\n",
    "                image_bytes = tf.io.serialize_tensor(image_tensor_f16)\n",
    "                feature = {\n",
    "                    'image': _bytes_feature(image_bytes),\n",
    "                    'label': _int64_feature(label.numpy())\n",
    "                }\n",
    "                num += 1\n",
    "                serialized_example = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "                writer.write(serialized_example)\n",
    "    print(f\"Processed and saved: {num} images\")\n",
    "\n",
    "def _parse_function(proto):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
    "    image_f16 = tf.io.parse_tensor(parsed_features['image'], out_type=tf.float16)\n",
    "    label = parsed_features['label']\n",
    "    image_f32 = tf.cast(image_f16, tf.float32)\n",
    "    image_f32.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
    "    return image_f32, label\n",
    "\n",
    "# Create adversarial dataset (uncomment to run generation)\n",
    "EPSILON = 0.03\n",
    "PGD_STEPS = 2\n",
    "PGD_STEP_SIZE = 0.007\n",
    "adversarial_test_file = '../datasets/adversaries/small_test_dataset.tfrec'\n",
    "adversarial_train_file = '../datasets/adversaries/small_train_dataset.tfrec'\n",
    "\n",
    "loaded_test_dataset = tf.data.TFRecordDataset(adversarial_test_file, compression_type='GZIP')\n",
    "parsed_test_dataset = loaded_test_dataset.map(_parse_function).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "base_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# print(\"Training baseline model...\\n\")\n",
    "# base_model.fit(train_dataset, verbose=1)\n",
    "\n",
    "print(\"Computing baseline metrics...\\n\")\n",
    "\n",
    "train_metrics = base_model.evaluate(train_dataset, verbose=1)\n",
    "test_metrics = base_model.evaluate(test_dataset, verbose=1)\n",
    "noisy_metrics = base_model.evaluate(parsed_test_dataset, verbose=1)\n",
    "\n",
    "# Extract metrics\n",
    "try:\n",
    "    metric_names = ['loss', 'accuracy', 'top_5_accuracy']\n",
    "    metrics_dict = {\n",
    "        'train': dict(zip(metric_names, train_metrics)),\n",
    "        'test': dict(zip(metric_names, test_metrics)),\n",
    "        'noisy': dict(zip(metric_names, noisy_metrics))\n",
    "    }\n",
    "\n",
    "    train_loss, train_acc, train_top5 = metrics_dict['train'].values()\n",
    "    test_loss, test_acc, test_top5 = metrics_dict['test'].values()\n",
    "    noisy_loss, noisy_acc, noisy_top5 = metrics_dict['noisy'].values()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting metrics: {e}\")\n",
    "\n",
    "print(\"## Base Performance Metrics 📊\")\n",
    "print(\"**Evaluated on clean test dataset**\\n\")\n",
    "print(f\"* **Top-1 Accuracy**: `{test_acc*100:.2f}%`\")\n",
    "print(f\"* **Top-5 Accuracy**: `{test_top5*100:.2f}%`\")\n",
    "print(f\"* **Loss**: `{test_loss:.3f}`\")\n",
    "print(\"\\n\" + \"---\")\n",
    "\n",
    "print(\"\\n## Generalization Analysis 🧠\")\n",
    "print(\"> *A smaller gap indicates better generalization*\")\n",
    "print(\"\\n\" + \"---\")\n",
    "\n",
    "print(\"\\n## Adversarial Robustness 🛡️\")\n",
    "print(\"**Evaluating model stability against adversarial inputs**\\n\")\n",
    "print(f\"* **Clean Data Accuracy**: `{test_acc*100:.2f}%`\")\n",
    "print(f\"* **Adversarial Data Accuracy**: `{noisy_acc*100:.2f}%`\")\n",
    "print(f\"* **Robustness Gap**: `{(test_acc-noisy_acc)*100:.2f}%`\")\n",
    "print(\"> *Smaller gaps between clean and adversarial metrics indicate better robustness*\")\n",
    "\n",
    "# Adversarial training\n",
    "loaded_train_dataset = tf.data.TFRecordDataset(adversarial_train_file, compression_type='GZIP')\n",
    "parsed_train_dataset = loaded_train_dataset.map(_parse_function).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Training robust model...\\n\")\n",
    "base_model.fit(parsed_train_dataset, verbose=1)\n",
    "\n",
    "train_metrics_adv = base_model.evaluate(parsed_train_dataset, verbose=1)\n",
    "test_metrics_adv = base_model.evaluate(test_dataset, verbose=1)\n",
    "noisy_metrics_adv = base_model.evaluate(parsed_test_dataset, verbose=1)\n",
    "\n",
    "try:\n",
    "    metrics_dict_adv = {\n",
    "        'train': dict(zip(metric_names, train_metrics_adv)),\n",
    "        'test': dict(zip(metric_names, test_metrics_adv)),\n",
    "        'noisy': dict(zip(metric_names, noisy_metrics_adv))\n",
    "    }\n",
    "\n",
    "    train_loss_adv, train_acc_adv, train_top5_adv = metrics_dict_adv['train'].values()\n",
    "    test_loss_adv, test_acc_adv, test_top5_adv = metrics_dict_adv['test'].values()\n",
    "    noisy_loss_adv, noisy_acc_adv, noisy_top5_adv = metrics_dict_adv['noisy'].values()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting metrics after adversarial training: {e}\")\n",
    "    print(\"Available metrics:\", base_model.metrics_names)\n",
    "    raise\n",
    "\n",
    "# Create figure for multiple plots - After adversarial training\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # Updated style for better visuals\n",
    "fig = plt.figure(figsize=(18, 6)) # Larger figure size\n",
    "\n",
    "# 1. Accuracy Comparison (Post-Adversarial Training)\n",
    "plt.subplot(1, 3, 1)\n",
    "metrics = ['Top-1 Acc', 'Top-5 Acc']\n",
    "clean_scores_post_adv = [test_acc_adv*100, test_top5_adv*100]\n",
    "noisy_scores_post_adv = [noisy_acc_adv*100, noisy_top5_adv*100]\n",
    "\n",
    "x = range(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], clean_scores_post_adv, width, label='Clean Data', color='mediumseagreen')\n",
    "plt.bar([i + width/2 for i in x], noisy_scores_post_adv, width, label='Adversarial Data', color='salmon')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Accuracy Comparison (Post-Adversarial Training)')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "plt.ylim(0, 100) # Set y-limit for better comparison\n",
    "\n",
    "# 2. Loss Comparison (Post-Adversarial Training)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(['Training', 'Testing', 'Adversarial'],\n",
    "        [train_loss_adv, test_loss_adv, noisy_loss_adv],\n",
    "        color=['steelblue', 'mediumseagreen', 'salmon'])\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Comparison Across Datasets (Post-Adversarial Training)')\n",
    "\n",
    "# 3. Robustness Gap (Post-Adversarial Training)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(['Generalization Gap', 'Robustness Gap'],\n",
    "        [(train_acc_adv - test_acc_adv)*100, (test_acc_adv - noisy_acc_adv)*100],\n",
    "        color=['steelblue', 'salmon'])\n",
    "plt.ylabel('Gap Percentage (%)')\n",
    "plt.title('Model Gaps Analysis (Post-Adversarial Training)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed metrics report (Post-Adversarial Training)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"       Comprehensive Model Evaluation Results (Post-Adversarial Training)\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"## Base Performance Metrics 📊\")\n",
    "print(\"**Evaluated on clean test dataset**\\n\")\n",
    "print(f\"* **Top-1 Accuracy**: `{test_acc_adv*100:.2f}%`\")\n",
    "print(f\"* **Top-5 Accuracy**: `{test_top5_adv*100:.2f}%`\")\n",
    "print(f\"* **Loss**: `{test_loss_adv:.3f}`\")\n",
    "print(\"\\n\" + \"---\")\n",
    "\n",
    "print(\"\\n## Generalization Analysis 🧠\")\n",
    "print(\"**Comparing training vs test performance**\\n\")\n",
    "print(f\"* **Training Accuracy**: `{train_acc_adv*100:.2f}%`\")\n",
    "print(f\"* **Test Accuracy**: `{test_acc_adv*100:.2f}%`\")\n",
    "print(f\"* **Generalization Gap**: `{(train_acc_adv-test_acc_adv)*100:.2f}%`\")\n",
    "print(f\"* **Training Loss**: `{train_loss_adv:.3f}`\")\n",
    "print(f\"* **Test Loss**: `{test_loss_adv:.3f}`\")\n",
    "print(\"> *A smaller gap indicates better generalization*\")\n",
    "print(\"\\n\" + \"---\")\n",
    "\n",
    "print(\"\\n## Adversarial Robustness 🛡️\")\n",
    "print(\"**Evaluating model stability against adversarial inputs**\\n\")\n",
    "print(f\"* **Clean Data Accuracy**: `{test_acc_adv*100:.2f}%`\")\n",
    "print(f\"* **Adversarial Data Accuracy**: `{noisy_acc_adv*100:.2f}%`\")\n",
    "print(f\"* **Robustness Gap**: `{(test_acc_adv-noisy_acc_adv)*100:.2f}%`\")\n",
    "print(\"> *Smaller gaps between clean and adversarial metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
