{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7dab9466",
      "metadata": {
        "id": "7dab9466"
      },
      "source": [
        "# Data Preprocessing Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8_n8JmyR-uhc",
      "metadata": {
        "id": "8_n8JmyR-uhc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # use \"3\" to hide even ERROR logs\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d4428a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d4428a8",
        "outputId": "f534e73a-cc60-449f-b586-d27c75a1d6f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Using GPU with 55GB of memory\n"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 200\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "EPOCHS = 5 # Changed to a lower number for demonstration if retraining is needed\n",
        "tf.random.set_seed(5)\n",
        "dataset_dir = \"../datasets\"\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "    dataset_dir = \"/content/drive/Othercomputers/Big Mac/datasets\"\n",
        "    BATCH_SIZE = 200\n",
        "\n",
        "    # Ensure the directory exists\n",
        "    tf.io.gfile.makedirs(dataset_dir)\n",
        "\n",
        "\n",
        "physical_gpus = tf.config.list_physical_devices('GPU')\n",
        "print(\"Available GPUs:\", physical_gpus)\n",
        "\n",
        "try:\n",
        "    tf.keras.mixed_precision.set_global_policy('float32') # Ensured float32 policy as per the original notebook\n",
        "    if physical_gpus: # Check if GPUs are available before setting virtual device\n",
        "        tf.config.experimental.set_virtual_device_configuration(\n",
        "            physical_gpus[0],\n",
        "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=56320)]  # Limit RAM to 55GB to avoid starving PC\n",
        "        )\n",
        "        print(\"Using GPU with 55GB of memory\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "x6x0_1gKGBqe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6x0_1gKGBqe",
        "outputId": "09c7d8a2-5705-45f2-bfcc-6888c9b51366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train image count: 3925\n",
            "Test image count: 3925\n"
          ]
        }
      ],
      "source": [
        "# Load ImageNet data\n",
        "def prepare_input_data(input):\n",
        "    image = tf.cast(input['image'], tf.float32)\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE)) # Corrected to use the casted image\n",
        "    image = preprocess_input(image)\n",
        "    label = input['label']\n",
        "    return image, label\n",
        "\n",
        "\n",
        "dataset, info = tfds.load(\n",
        "    'imagenette',\n",
        "    shuffle_files=False,\n",
        "    with_info=True,\n",
        "    data_dir=dataset_dir\n",
        ")\n",
        "\n",
        "\n",
        "# dataset, info = tfds.load(\n",
        "#     'imagenet2012',\n",
        "#     shuffle_files=False,\n",
        "#     with_info=True,\n",
        "#     data_dir=dataset_dir\n",
        "# )\n",
        "\n",
        "train_dataset_image_count = info.splits[\"train\"].num_examples\n",
        "test_dataset_image_count = info.splits[\"validation\"].num_examples\n",
        "\n",
        "print(f'Train image count: {test_dataset_image_count}')\n",
        "print(f'Test image count: {test_dataset_image_count}')\n",
        "\n",
        "train_dataset = dataset['train'].map(prepare_input_data, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "test_dataset = dataset['validation'].map(prepare_input_data, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7dbe3779",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dbe3779",
        "outputId": "8292b727-b955-43b5-c2ed-4f4120d584ca"
      },
      "outputs": [],
      "source": [
        "# Load ResNet50 model\n",
        "base_model = ResNet50(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    pooling=None,\n",
        "    classes=10,\n",
        "    classifier_activation='softmax'\n",
        ")\n",
        "\n",
        "# base_model = ResNet50(\n",
        "#     include_top=True,\n",
        "#     weights='imagenet',\n",
        "#     input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "#     pooling=None,\n",
        "#     classes=1000,\n",
        "#     classifier_activation='softmax'\n",
        "# )\n",
        "\n",
        "# Functions for adversarial data generation and loading\n",
        "def _bytes_feature(value):\n",
        "    if isinstance(value, type(tf.constant(0))):\n",
        "        value = value.numpy()\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def _create_adversary_with_pgd(model, images, labels, eps, eps_iter, nb_iter):\n",
        "    x_adv = tf.identity(images)\n",
        "    # Use from_logits=False because classifier_activation='softmax' means model outputs probabilities\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "    for _ in range(nb_iter):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x_adv)\n",
        "            prediction = model(x_adv, training=False)\n",
        "            loss = loss_object(labels, prediction)\n",
        "\n",
        "        gradients = tape.gradient(loss, x_adv)\n",
        "        signed_grad = tf.sign(gradients)\n",
        "        x_adv = x_adv + eps_iter * signed_grad\n",
        "        perturbation = tf.clip_by_value(x_adv - images, -eps, eps)\n",
        "        x_adv = images + perturbation\n",
        "\n",
        "    return x_adv\n",
        "\n",
        "\n",
        "def generate_adversarial_dataset_in_batches(folder, dataset, model, eps, pgd_steps, pgd_step_size, count):\n",
        "    options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "    image_count = 0\n",
        "    dataset_iterator = iter(dataset)\n",
        "    batch_count = math.ceil(count / BATCH_SIZE)\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataset_iterator):\n",
        "        batch_no = f\"{folder}-record-{i}-of-{batch_count}.tfrec\"\n",
        "        print(f\"batch_{i}\")\n",
        "        adv_images = _create_adversary_with_pgd(\n",
        "            model=model,\n",
        "            images=images,\n",
        "            labels=labels,\n",
        "            eps=eps,\n",
        "            eps_iter=pgd_step_size,\n",
        "            nb_iter=pgd_steps\n",
        "        )\n",
        "\n",
        "        with tf.io.TFRecordWriter(batch_no, options=options) as writer:\n",
        "            for i in range(len(adv_images)):\n",
        "                image_tensor = adv_images[i]\n",
        "                label = labels[i]\n",
        "                image_tensor_f16 = tf.cast(image_tensor, tf.float16)\n",
        "                image_bytes = tf.io.serialize_tensor(image_tensor_f16)\n",
        "                feature = {\n",
        "                    'image': _bytes_feature(image_bytes),\n",
        "                    'label': _int64_feature(label.numpy())\n",
        "                }\n",
        "                image_count += 1\n",
        "                serialized_example = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
        "                writer.write(serialized_example)\n",
        "    print(f\"Processed and saved: {image_count} images\")\n",
        "\n",
        "def _parse_function(proto):\n",
        "    feature_description = {\n",
        "        'image': tf.io.FixedLenFeature([], tf.string),\n",
        "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "    parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
        "    image_f16 = tf.io.parse_tensor(parsed_features['image'], out_type=tf.float16)\n",
        "    label = parsed_features['label']\n",
        "    image_f32 = tf.cast(image_f16, tf.float32)\n",
        "    image_f32.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
        "    return image_f32, label\n",
        "\n",
        "base_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy'),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fBYFgK2mfy6t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBYFgK2mfy6t",
        "outputId": "689d52fc-10a4-4d13-992d-c0598f115849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training base model...\n",
            "\n",
            "Epoch 1/4\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 4s/step - accuracy: 0.3177 - loss: 2.3310 - top_5_accuracy: 0.7789\n",
            "Epoch 2/4\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 3s/step - accuracy: 0.4513 - loss: 1.6647 - top_5_accuracy: 0.8840\n",
            "Epoch 3/4\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3s/step - accuracy: 0.5638 - loss: 1.3673 - top_5_accuracy: 0.9280\n",
            "Epoch 4/4\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3s/step - accuracy: 0.5989 - loss: 1.2525 - top_5_accuracy: 0.9372\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x40ac7eae0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Training base model...\\n\")\n",
        "base_model.fit(train_dataset, verbose=1, batch_size=BATCH_SIZE, epochs=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "lfBX8E_Nf4cu",
      "metadata": {
        "id": "lfBX8E_Nf4cu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generate adversarial test data\n",
            "batch_0\n",
            "batch_1\n",
            "batch_2\n",
            "batch_3\n",
            "batch_4\n",
            "batch_5\n",
            "batch_6\n",
            "batch_7\n",
            "batch_8\n",
            "batch_9\n",
            "batch_10\n",
            "batch_11\n",
            "batch_12\n",
            "batch_13\n",
            "batch_14\n",
            "batch_15\n",
            "batch_16\n",
            "batch_17\n",
            "batch_18\n",
            "batch_19\n",
            "Processed and saved: 3925 images\n",
            "Generate adversarial train data\n",
            "batch_0\n",
            "batch_1\n",
            "batch_2\n",
            "batch_3\n",
            "batch_4\n",
            "batch_5\n",
            "batch_6\n",
            "batch_7\n",
            "batch_8\n",
            "batch_9\n",
            "batch_10\n",
            "batch_11\n",
            "batch_12\n",
            "batch_13\n",
            "batch_14\n",
            "batch_15\n",
            "batch_16\n",
            "batch_17\n",
            "batch_18\n",
            "batch_19\n",
            "batch_20\n",
            "batch_21\n",
            "batch_22\n",
            "batch_23\n",
            "batch_24\n",
            "batch_25\n",
            "batch_26\n",
            "batch_27\n",
            "batch_28\n",
            "batch_29\n",
            "batch_30\n",
            "batch_31\n",
            "batch_32\n",
            "batch_33\n",
            "batch_34\n",
            "batch_35\n",
            "batch_36\n",
            "batch_37\n",
            "batch_38\n",
            "batch_39\n",
            "batch_40\n",
            "batch_41\n",
            "batch_42\n",
            "batch_43\n",
            "batch_44\n",
            "batch_45\n",
            "batch_46\n",
            "batch_47\n",
            "Processed and saved: 9469 images\n"
          ]
        }
      ],
      "source": [
        "# Create adversarial dataset\n",
        "EPSILON = 8/255 # Maximum allowed change.\n",
        "PGD_STEPS = 5 # use 5 steps\n",
        "PGD_STEP_SIZE = 2/255 # change by much at each step\n",
        "# adversarial_test_file = f'{dataset_dir}/adversaries/test_dataset.tfrec'\n",
        "test_dataset_folder = f'{dataset_dir}/adversaries/imagenette/test'\n",
        "print(\"Generate adversarial test data\")\n",
        "generate_adversarial_dataset_in_batches(\n",
        "    folder=test_dataset_folder,\n",
        "    dataset=test_dataset,\n",
        "    model=base_model,\n",
        "    eps=EPSILON,\n",
        "    pgd_steps=PGD_STEPS,\n",
        "    pgd_step_size=PGD_STEP_SIZE,\n",
        "    count=test_dataset_image_count,\n",
        ")\n",
        "\n",
        "train_dataset_folder = f'{dataset_dir}/adversaries/imagenette/train'\n",
        "print(\"Generate adversarial train data\")\n",
        "generate_adversarial_dataset_in_batches(\n",
        "    folder=train_dataset_folder,\n",
        "    dataset=train_dataset,\n",
        "    model=base_model,\n",
        "    eps=EPSILON,\n",
        "    pgd_steps=PGD_STEPS,\n",
        "    pgd_step_size=PGD_STEP_SIZE,\n",
        "    count=train_dataset_image_count,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8613b641",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found the following TFRecord files: ['../datasets/adversaries/small_test/batch_11.tfrec', '../datasets/adversaries/small_test/batch_13.tfrec', '../datasets/adversaries/small_test/batch_8.tfrec', '../datasets/adversaries/small_test/batch_17.tfrec', '../datasets/adversaries/small_test/batch_15.tfrec', '../datasets/adversaries/small_test/batch_12.tfrec', '../datasets/adversaries/small_test/batch_9.tfrec', '../datasets/adversaries/small_test/batch_10.tfrec', '../datasets/adversaries/small_test/batch_14.tfrec', '../datasets/adversaries/small_test/batch_16.tfrec', '../datasets/adversaries/small_test/batch_1.tfrec', '../datasets/adversaries/small_test/batch_3.tfrec', '../datasets/adversaries/small_test/batch_18.tfrec', '../datasets/adversaries/small_test/batch_7.tfrec', '../datasets/adversaries/small_test/batch_5.tfrec', '../datasets/adversaries/small_test/batch_2.tfrec', '../datasets/adversaries/small_test/batch_19.tfrec', '../datasets/adversaries/small_test/batch_4.tfrec', '../datasets/adversaries/small_test/batch_20.tfrec', '../datasets/adversaries/small_test/batch_6.tfrec']\n",
            "Dataset cardinality is unknown. Counting via iteration...\n",
            "Total number of images in the dataset: 3925\n"
          ]
        }
      ],
      "source": [
        "# import tensorflow as tf\n",
        "# import glob\n",
        "\n",
        "# # Use glob to get a list of all TFRecord files\n",
        "# file_paths = glob.glob(f'{dataset_dir}/adversaries/small_test/batch_*.tfrec') \n",
        "# print(f\"Found the following TFRecord files: {file_paths}\")\n",
        "\n",
        "# # Create a TFRecordDataset from the list of file paths\n",
        "# # The files are interleaved automatically for better performance\n",
        "# raw_dataset = tf.data.TFRecordDataset(file_paths,  compression_type='GZIP')\n",
        "\n",
        "# ## Load data from file\n",
        "# def _parse_function(proto):\n",
        "#     \"\"\"\n",
        "#     Parses a single example proto by deserializing the float16 tensor\n",
        "#     and casting it back to float32.\n",
        "#     \"\"\"\n",
        "#     feature_description = {\n",
        "#         'image': tf.io.FixedLenFeature([], tf.string),\n",
        "#         'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "#     }\n",
        "#     parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
        "\n",
        "#     # 1. Deserialize the byte string back into a float16 tensor\n",
        "#     image_f16 = tf.io.parse_tensor(parsed_features['image'], out_type=tf.float16)\n",
        "#     label = parsed_features['label']\n",
        "\n",
        "#     # 2. Cast the image back to float32 for the model\n",
        "#     image_f32 = tf.cast(image_f16, tf.float32)\n",
        "\n",
        "#     # 3. Set the shape on the final float32 tensor\n",
        "#     image_f32.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
        "\n",
        "#     return image_f32, label\n",
        "\n",
        "# # Load the TFRecord file back into a dataset\n",
        "# # loaded_test_dataset = tf.data.TFRecordDataset(adversarial_test_file, compression_type='GZIP')\n",
        "\n",
        "# parsed_test_dataset = raw_dataset.map(_parse_function).batch(200).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# def count_images_in_dataset(dataset):\n",
        "#     \"\"\"\n",
        "#     Counts the number of images in an unbatched TF.data.Dataset.\n",
        "    \n",
        "#     Args:\n",
        "#         dataset: The unbatched tf.data.Dataset containing the images.\n",
        "        \n",
        "#     Returns:\n",
        "#         The total number of images as an integer.\n",
        "#     \"\"\"\n",
        "#     # Use cardinality() to get the number of elements.\n",
        "#     # This is the most efficient method as it doesn't require iterating.\n",
        "#     count = tf.data.experimental.cardinality(dataset).numpy()\n",
        "\n",
        "#     # If cardinality is unknown, fall back to iterating\n",
        "#     if count == tf.data.experimental.UNKNOWN_CARDINALITY:\n",
        "#         print(\"Dataset cardinality is unknown. Counting via iteration...\")\n",
        "#         count = sum(1 for _ in dataset)\n",
        "    \n",
        "#     return count\n",
        "\n",
        "\n",
        "# # Use the function to count the images in the raw, unbatched dataset\n",
        "# total_images = count_images_in_dataset(raw_dataset)\n",
        "# print(f\"Total number of images in the dataset: {total_images}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
